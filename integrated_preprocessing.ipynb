{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c04b0f8e",
   "metadata": {},
   "source": [
    "# IntegratedHTMLPreprocessor 클래스 기능 상세 설명\n",
    "\n",
    "이 클래스는 HTML 감사보고서 파일을 일괄적으로 전처리하여 구조를 정규화하고, 숫자 및 섹션 정보를 표준화합니다. 아래는 각 기능과 단계별 처리 내역입니다.\n",
    "\n",
    "## 1. 구조 정규화\n",
    "- **불필요 태그 제거**: `<script>`, `<style>`, `<noscript>` 등 스킵 태그를 모두 삭제합니다.\n",
    "- **테이블 구조 정규화**: 클래스가 'table'인 테이블만 처리 대상으로 삼고, 나머지는 무시합니다.\n",
    "- **텍스트 및 태그 스트림화**: HTML을 순회하며 텍스트와 주요 태그를 선형 스트림으로 변환합니다.\n",
    "- **빈 태그 및 불필요 공백 정리**: 텍스트 내 불필요한 공백 및 빈 태그를 제거합니다.\n",
    "\n",
    "## 2. 섹션 및 계층 구조 태깅\n",
    "- **섹션 태그 래핑**: 주요 섹션(감사보고서, 재무제표, 주석, 감사의견 등)을 `<SEC1>`, `<SEC2>`, `<SEC3>`, `<SEC4>`로 래핑합니다.\n",
    "- **섹션 제목 자동 추출 및 태깅**: 각 섹션의 제목을 자동으로 추출하여 `<SEC-title>`로 태깅합니다.\n",
    "- **SEC1 하위 섹션 분리**: SEC1(감사보고서) 내 주요 하위 제목을 기준으로 `<SEC1-1>`, `<SEC1-2>` 등으로 분리합니다.\n",
    "- **SEC2 테이블 드롭**: SEC2(재무제표) 내 처음 5개의 테이블을 삭제합니다.\n",
    "- **SEC4 테이블 전체 드롭**: SEC4(감사의견) 내 모든 테이블을 삭제합니다.\n",
    "\n",
    "## 3. 계층별 텍스트 강조 및 구조화\n",
    "- **SEC1 하위 제목 강조**: 하위 제목에 `<big>` 태그를 추가하여 강조합니다.\n",
    "- **SEC3 계층 구조 태깅**: SEC3(주석) 내 대제목, 중제목, 소제목을 각각 `<big>`, `<mid>`, `<small>`로 태깅하고, 계층별로 `<SEC3-x>`, `<SEC3-x.y>`, `<SEC3-x.y.z>`로 래핑합니다.\n",
    "- **SEC4 주요 제목 강조**: SEC4 내 주요 제목에 `<big>` 태그를 추가합니다.\n",
    "\n",
    "## 4. 숫자 및 단위 정규화\n",
    "- **테이블 내 숫자 정규화**:\n",
    "    - 쉼표 제거: \"1,234\" → \"1234\"\n",
    "    - 괄호 숫자 음수 변환: \"(123)\" → \"-123\"\n",
    "    - 대시(–, —, -)는 \"NA\"로 변환\n",
    "- **텍스트 노드 내 숫자 정규화**: 테이블 외부 텍스트에서도 쉼표가 포함된 숫자를 정규화합니다.\n",
    "\n",
    "## 5. 파일 및 디렉토리 일괄 처리\n",
    "- **단일 파일 처리**: `process_file()` 메서드로 개별 HTML 파일을 전처리하여 결과를 저장합니다.\n",
    "- **디렉토리 일괄 처리**: `process_directory()` 메서드로 지정된 폴더 내 모든 HTML 파일을 일괄 처리합니다.\n",
    "- **출력 파일 저장**: 전처리 결과는 `preprocessed` 폴더에 `_final.html`로 저장됩니다.\n",
    "\n",
    "## 6. 기타 기능\n",
    "- **헤더/테일 트리밍**: 불필요한 앞부분과 \"외부감사 실시내용\" 이후 뒷부분을 자동으로 잘라냅니다.\n",
    "- **예외 처리 및 로깅**: 처리 중 오류 발생 시 파일별로 실패 내역을 출력합니다.\n",
    "\n",
    "---\n",
    "### 전체 처리 흐름\n",
    "1. HTML 파일 로드 및 파싱\n",
    "2. 불필요 태그 제거\n",
    "3. 테이블 및 텍스트 숫자 정규화\n",
    "4. 선형 스트림 변환 및 구조 정리\n",
    "5. 섹션 및 계층 구조 태깅\n",
    "6. 강조 및 래핑 처리\n",
    "7. 결과 HTML 파일 저장\n",
    "\n",
    "이 클래스 하나로 감사보고서 HTML 파일의 구조, 숫자, 섹션, 계층, 불필요 요소 제거 등 모든 전처리 과정을 자동화할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10d05f9",
   "metadata": {},
   "source": [
    "# 전처리 단계별 설명\n",
    "\n",
    "1. **구조 정규화**\n",
    "   - 섹션 태그 추가 (`<section>`, `<subsection>` 등)\n",
    "   - 계층 구조 명확화\n",
    "   - 테이블 구조 정규화\n",
    "\n",
    "2. **숫자 정규화**\n",
    "   - 쉼표 제거 (예: \"1,234\" → \"1234\")\n",
    "   - 괄호 숫자 음수 변환 (예: \"(123)\" → \"-123\")\n",
    "   - 단위 정규화 (예: \"백만원\" → \"000000\")\n",
    "\n",
    "3. **불필요 요소 제거**\n",
    "   - 스타일 속성 제거\n",
    "   - 빈 태그 제거\n",
    "   - 불필요한 공백 정리\n",
    "\n",
    "각 단계는 `process_file()` 메서드에서 순차적으로 실행됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58e5986c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "from pathlib import Path\n",
    "import html, re, os\n",
    "\n",
    "class IntegratedHTMLPreprocessor:\n",
    "    # === 설정 ===\n",
    "    KEEP_TAGS   = {\"table\",\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"}\n",
    "    BLOCK_BREAK = {\"p\",\"div\",\"li\",\"ul\",\"ol\",\"section\",\"article\",\"header\",\"footer\",\"aside\",\"address\",\"pre\",\"blockquote\",\"tr\"}\n",
    "    SKIP_TAGS   = {\"script\",\"style\",\"noscript\"}\n",
    "    SKIP_CONTAINERS = {\"script\",\"style\",\"noscript\"}\n",
    "    SECTION_NAMES = {1:\"감사보고서\", 2:\"재무제표\", 3:\"주석\", 4:\"감사의견\"}\n",
    "    SEC1_SUBHEADINGS = {\n",
    "        \"재무제표에 대한 경영진의 책임\",\"감사인의 책임\",\"감사의견\",\"기타사항\",\"핵심감사사항\",\n",
    "        \"재무제표에 대한 경영진과 지배기구의 책임\",\"재무제표감사에 대한 감사인의 책임\",\n",
    "    }\n",
    "    SEC4_HEADINGS = {\n",
    "        \"내부회계관리제도에 대한 감사의견\",\n",
    "        \"내부회계관리제도 감사의견근거\",\n",
    "        \"내부회계관리제도에 대한 경영진과 지배기구의 책임\",\n",
    "        \"내부회계관리제도감사에 대한 감사인의 책임\",\n",
    "        \"내부회계관리제도의 정의와 고유한계\",\n",
    "        \"내부회계관리제도 검토의견\",\n",
    "    }\n",
    "    RE_P_CONT_HEADING = re.compile(r\"\"\"^\\s*[\"“”]?\\d{1,3}(?:\\s*[.)])?\\s*[^:\\n]*?,?\\s*계\\s*속\\s*[:：;]?\\s*[\"“”]?\\s*$\"\"\", re.X)\n",
    "    RE_SOLO_CONT_LINE = re.compile(r\"\"\"^\\s*[\"“”']?계\\s*속\\s*[:：;]?\\s*[\"“”']?\\s*$\"\"\", re.X)\n",
    "    RE_NUM_ONLY       = re.compile(r\"^\\s*[\\d,]+(?:\\.\\d+)?%?\\s*$\")\n",
    "    RE_MAJOR   = re.compile(r\"\"\"^\\s*(?P<maj>\\d{1,3})\\.(?!\\d)\\s*(?P<title>(?=.*[가-힣A-Za-z]).+?)(?:,?\\s*계\\s*속)?\\s*[:：]?\\s*$\"\"\", re.X)\n",
    "    RE_MID_NUM = re.compile(r\"\"\"^\\s*(?P<maj>\\d{1,3})\\.\\s*(?P<sub>\\d{1,3})\\s+(?P<title>(?=.*[가-힣A-Za-z]).+?)(?:,?\\s*계\\s*속)?\\s*[:：]?\\s*$\"\"\", re.X)\n",
    "    RE_LETTER  = re.compile(r\"\"\"^\\s*(?:\\(|\\[)?(?P<enum>[가-힣A-Za-z])(?:\\)|\\])?\\.\\s+(?P<title>(?=.*[가-힣A-Za-z]).+?)\\s*$\"\"\", re.X)\n",
    "    LETTER_SEQ   = ['가','나','다','라','마','바','사','아','자','차','카','타','파','하']\n",
    "    LETTER_INDEX = {ch:i for i,ch in enumerate(LETTER_SEQ)}\n",
    "    RE_NUM_WITH_COMMAS   = re.compile(r\"^\\s*\\d[\\d,]*\\s*$\")\n",
    "    RE_PARENS_NUM        = re.compile(r\"^\\s*\\(\\s*(\\d[\\d,]*)\\s*\\)\\s*$\")\n",
    "    RE_JUST_DASH         = re.compile(r\"^\\s*[–—-]\\s*$\")\n",
    "    RE_COMMAS_IN_NUMBER  = re.compile(r\"(?<=\\d),(?=\\d)\")\n",
    "\n",
    "    def normalize_ws(self, s: str) -> str:\n",
    "        s = (s or \"\").replace(\"\\xa0\", \" \")\n",
    "        s = re.sub(r\"[ \\t]+\", \" \", s)\n",
    "        return s\n",
    "\n",
    "    def _text_from_outer_html(self, v_html: str) -> str:\n",
    "        try:\n",
    "            bs = BeautifulSoup(v_html, \"html.parser\")\n",
    "            p = bs.find(\"p\")\n",
    "            if p:\n",
    "                return self.normalize_ws(p.get_text(strip=True))\n",
    "            return self.normalize_ws(bs.get_text(strip=True))\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "\n",
    "    def is_continuation_p(self, text_line: str) -> bool:\n",
    "        s = self.normalize_ws(text_line).strip().strip('\"“”')\n",
    "        return bool(self.RE_P_CONT_HEADING.match(s))\n",
    "    def is_solo_continuation(self, text_line: str) -> bool:\n",
    "        s = self.normalize_ws(text_line).strip()\n",
    "        return bool(self.RE_SOLO_CONT_LINE.match(s))\n",
    "    def is_num_only_line(self, s: str) -> bool:\n",
    "        return bool(self.RE_NUM_ONLY.match(self.normalize_ws(s)))\n",
    "\n",
    "    def has_section_class(self, tag: Tag) -> bool:\n",
    "        classes = [str(c).lower() for c in (tag.get(\"class\") or [])]\n",
    "        return any(\"section\" in c for c in classes)\n",
    "\n",
    "    def _soup_first_elem(self, html_str: str):\n",
    "        try:\n",
    "            return BeautifulSoup(html_str, \"html.parser\").find(True, recursive=True)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def _has_section_in_outer_html(self, html_str: str) -> bool:\n",
    "        el = self._soup_first_elem(html_str)\n",
    "        if not el: return False\n",
    "        classes = [c.lower() for c in (el.get(\"class\") or [])]\n",
    "        return any(\"section\" in c for c in classes)\n",
    "\n",
    "    def _is_table_html(self, s: str) -> bool:\n",
    "        return isinstance(s, str) and s.lstrip().lower().startswith(\"<table\")\n",
    "\n",
    "    def traverse_stream(self, node: Tag):\n",
    "        for child in getattr(node, \"children\", []):\n",
    "            if isinstance(child, NavigableString):\n",
    "                txt = self.normalize_ws(str(child))\n",
    "                if txt: yield (\"text\", txt)\n",
    "                continue\n",
    "            if not isinstance(child, Tag):\n",
    "                continue\n",
    "            name = (child.name or \"\").lower()\n",
    "            if name in self.SKIP_TAGS:\n",
    "                continue\n",
    "            if name == \"br\":\n",
    "                yield (\"text\", \"\\n\");  continue\n",
    "            if name in {\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"} and self.has_section_class(child):\n",
    "                if not child.get_text(strip=True):\n",
    "                    continue\n",
    "            if self.has_section_class(child):\n",
    "                yield (\"keep\", str(child));  continue\n",
    "            if name in self.KEEP_TAGS:\n",
    "                yield (\"keep\", str(child));  continue\n",
    "            yield from self.traverse_stream(child)\n",
    "            if name in self.BLOCK_BREAK:\n",
    "                yield (\"text\", \"\\n\")\n",
    "\n",
    "    def flatten_to_linear(self, soup: BeautifulSoup):\n",
    "        body = soup.body or soup\n",
    "        tokens = list(self.traverse_stream(body))\n",
    "        linear = []\n",
    "        buf = \"\"\n",
    "        def flush_buf():\n",
    "            nonlocal buf\n",
    "            if not buf: return\n",
    "            for line in buf.split(\"\\n\"):\n",
    "                line = self.normalize_ws(line).strip()\n",
    "                if not line: continue\n",
    "                if self.is_continuation_p(line) or self.is_solo_continuation(line):\n",
    "                    continue\n",
    "                linear.append((\"p\", f\"<p>{html.escape(line)}</p>\"))\n",
    "            buf = \"\"\n",
    "        for kind, payload in tokens:\n",
    "            if kind == \"text\":\n",
    "                buf += payload\n",
    "            else:\n",
    "                flush_buf()\n",
    "                linear.append((\"keep\", payload))\n",
    "        flush_buf()\n",
    "        return linear\n",
    "\n",
    "    def trim_head_linear(self, linear):\n",
    "        first_sec_idx = next((i for i,(k,v) in enumerate(linear) if k==\"keep\" and self._has_section_in_outer_html(v)), None)\n",
    "        if first_sec_idx is None: return linear\n",
    "        prev_p_idx = None\n",
    "        for i in range(first_sec_idx-1, -1, -1):\n",
    "            if linear[i][0] == \"p\":\n",
    "                prev_p_idx = i; break\n",
    "        start_idx = prev_p_idx if prev_p_idx is not None else first_sec_idx\n",
    "        return linear[start_idx:]\n",
    "\n",
    "    def cut_tail_linear(self, linear, stop_phrase=\"외부감사 실시내용\"):\n",
    "        for i,(k,v) in enumerate(linear):\n",
    "            if k==\"p\" and stop_phrase in self._text_from_outer_html(v):\n",
    "                return linear[:i]\n",
    "        return linear\n",
    "\n",
    "    def wrap_sections_linear(self, linear):\n",
    "        out = []\n",
    "        sec_idx = 0\n",
    "        sec_open = False\n",
    "        for k, v in linear:\n",
    "            if k==\"keep\" and self._has_section_in_outer_html(v):\n",
    "                if sec_open:\n",
    "                    out.append((None, f\"</SEC{sec_idx}>\"))\n",
    "                sec_idx += 1\n",
    "                sec_name = self.SECTION_NAMES.get(sec_idx, f\"섹션{sec_idx}\")\n",
    "                out.append((None, f'<SEC{sec_idx} name=\"{sec_name}\">'))\n",
    "                sec_open = True\n",
    "                out.append((k, v))\n",
    "            else:\n",
    "                out.append((k, v))\n",
    "        if sec_open:\n",
    "            out.append((None, f\"</SEC{sec_idx}>\"))\n",
    "        return out\n",
    "\n",
    "    def split_sec1_subsections_and_drop_tables(self, sec_wrapped):\n",
    "        open_i = close_i = None\n",
    "        for i, (k, v) in enumerate(sec_wrapped):\n",
    "            if k is None and isinstance(v, str) and v.startswith(\"<SEC1 \"):\n",
    "                open_i = i; break\n",
    "        if open_i is None: return sec_wrapped\n",
    "        for j in range(open_i+1, len(sec_wrapped)):\n",
    "            if sec_wrapped[j][0] is None and isinstance(sec_wrapped[j][1], str) and sec_wrapped[j][1].strip() == \"</SEC1>\":\n",
    "                close_i = j; break\n",
    "        if close_i is None: return sec_wrapped\n",
    "        before = sec_wrapped[:open_i+1]\n",
    "        inside = sec_wrapped[open_i+1:close_i]\n",
    "        after  = sec_wrapped[close_i:]\n",
    "        out = []\n",
    "        sub_idx = 0\n",
    "        sub_open = False\n",
    "        for k, v in inside:\n",
    "            if self._is_table_html(v):\n",
    "                continue\n",
    "            if k == \"p\":\n",
    "                txt = self._text_from_outer_html(v)\n",
    "                if txt in self.SEC1_SUBHEADINGS:\n",
    "                    if sub_open:\n",
    "                        out.append((None, f\"</SEC1-{sub_idx}>\"))\n",
    "                    sub_idx += 1\n",
    "                    out.append((None, f'<SEC1-{sub_idx} name=\"{html.escape(txt)}\">'))\n",
    "                    sub_open = True\n",
    "                    out.append((k, v))\n",
    "                    continue\n",
    "            out.append((k, v))\n",
    "        if sub_open:\n",
    "            out.append((None, f\"</SEC1-{sub_idx}>\"))\n",
    "        return before + out + after\n",
    "\n",
    "    def drop_first_n_tables_in_sec2(self, linear_pairs, n=5):\n",
    "        open_i = close_i = None\n",
    "        for i,(k,v) in enumerate(linear_pairs):\n",
    "            if k is None and isinstance(v,str) and v.startswith(\"<SEC2 \"):\n",
    "                open_i = i; break\n",
    "        if open_i is None: return linear_pairs\n",
    "        for j in range(open_i+1,len(linear_pairs)):\n",
    "            if linear_pairs[j][0] is None and isinstance(linear_pairs[j][1],str) and linear_pairs[j][1].strip()==\"</SEC2>\":\n",
    "                close_i = j; break\n",
    "        if close_i is None: return linear_pairs\n",
    "        before = linear_pairs[:open_i+1]\n",
    "        inside = linear_pairs[open_i+1:close_i]\n",
    "        after  = linear_pairs[close_i:]\n",
    "        out = []\n",
    "        dropped = 0\n",
    "        for k,v in inside:\n",
    "            if dropped < n and self._is_table_html(v):\n",
    "                dropped += 1\n",
    "                continue\n",
    "            out.append((k,v))\n",
    "        return before + out + after\n",
    "\n",
    "    def drop_all_tables_in_sec4(self, linear_pairs):\n",
    "        open_i = close_i = None\n",
    "        for i,(k,v) in enumerate(linear_pairs):\n",
    "            if k is None and isinstance(v,str) and v.startswith(\"<SEC4 \"):\n",
    "                open_i = i; break\n",
    "        if open_i is None: return linear_pairs\n",
    "        for j in range(open_i+1,len(linear_pairs)):\n",
    "            if linear_pairs[j][0] is None and isinstance(linear_pairs[j][1],str) and linear_pairs[j][1].strip()==\"</SEC4>\":\n",
    "                close_i = j; break\n",
    "        if close_i is None: return linear_pairs\n",
    "        before = linear_pairs[:open_i+1]\n",
    "        inside = linear_pairs[open_i+1:close_i]\n",
    "        after  = linear_pairs[close_i:]\n",
    "        out = [(k,v) for (k,v) in inside if not self._is_table_html(v)]\n",
    "        return before + out + after\n",
    "\n",
    "    def wrap_sec_titles(self, linear_pairs):\n",
    "        out = []\n",
    "        i = 0\n",
    "        while i < len(linear_pairs):\n",
    "            k, v = linear_pairs[i]\n",
    "            out.append((k, v))\n",
    "            if k is None and isinstance(v, str) and v.startswith(\"<SEC\") and not v.startswith(\"<SEC-title\"):\n",
    "                j = i + 1\n",
    "                if j < len(linear_pairs):\n",
    "                    k2, v2 = linear_pairs[j]\n",
    "                    if k2 == \"keep\" and self._has_section_in_outer_html(v2):\n",
    "                        out.append((None, f\"<SEC-title>{v2}</SEC-title>\"))\n",
    "                        i = j\n",
    "            i += 1\n",
    "        return out\n",
    "\n",
    "    def add_big_to_sec1_subheading_ps(self, linear_pairs):\n",
    "        out = []\n",
    "        inside_sec1_sub = False\n",
    "        for k, v in linear_pairs:\n",
    "            if k is None and isinstance(v, str):\n",
    "                if v.startswith(\"<SEC1-\"): inside_sec1_sub = True\n",
    "                elif v.strip().startswith(\"</SEC1-\"): inside_sec1_sub = False\n",
    "                out.append((k, v)); continue\n",
    "            if inside_sec1_sub and k == \"p\":\n",
    "                txt = self._text_from_outer_html(v)\n",
    "                if txt in self.SEC1_SUBHEADINGS:\n",
    "                    out.append((k, f\"<big>{v}</big>\")); continue\n",
    "            out.append((k, v))\n",
    "        return out\n",
    "\n",
    "    def tag_sec3_headings(self, linear_pairs):\n",
    "        out = []\n",
    "        inside_sec3 = False\n",
    "        parent_state = 'none'\n",
    "        mid_letter_idx = None\n",
    "        small_letter_idx = None\n",
    "        for k, v in linear_pairs:\n",
    "            if k is None and isinstance(v, str):\n",
    "                if v.startswith(\"<SEC3 \"):\n",
    "                    inside_sec3 = True\n",
    "                    parent_state = 'none'; mid_letter_idx = None; small_letter_idx = None\n",
    "                    out.append((k, v)); continue\n",
    "                if v.strip() == \"</SEC3>\":\n",
    "                    inside_sec3 = False\n",
    "                    parent_state = 'none'; mid_letter_idx = None; small_letter_idx = None\n",
    "                    out.append((k, v)); continue\n",
    "            if not inside_sec3 or k != \"p\":\n",
    "                out.append((k, v)); continue\n",
    "            text = self._text_from_outer_html(v)\n",
    "            if not text or self.is_continuation_p(text) or self.is_num_only_line(text):\n",
    "                out.append((k, v)); continue\n",
    "            if self.RE_MID_NUM.match(text):\n",
    "                out.append((k, f\"<mid>{v}</mid>\"))\n",
    "                parent_state = 'mid_num'\n",
    "                small_letter_idx = None\n",
    "                continue\n",
    "            if self.RE_MAJOR.match(text):\n",
    "                out.append((k, f\"<big>{v}</big>\"))\n",
    "                parent_state = 'big'\n",
    "                mid_letter_idx = None; small_letter_idx = None\n",
    "                continue\n",
    "            m = self.RE_LETTER.match(text)\n",
    "            if m:\n",
    "                ch = m.group(\"enum\")[0]\n",
    "                idx = self.LETTER_INDEX.get(ch, None)\n",
    "                if parent_state == 'mid_num':\n",
    "                    if idx is None:\n",
    "                        out.append((k, v)); continue\n",
    "                    if small_letter_idx is None:\n",
    "                        if idx == 0:\n",
    "                            out.append((k, f\"<small>{v}</small>\")); small_letter_idx = 1\n",
    "                        else:\n",
    "                            out.append((k, v))\n",
    "                    else:\n",
    "                        if idx == small_letter_idx:\n",
    "                            out.append((k, f\"<small>{v}</small>\")); small_letter_idx += 1\n",
    "                        else:\n",
    "                            out.append((k, v))\n",
    "                    continue\n",
    "                if parent_state in ('big','mid_letter'):\n",
    "                    if idx is None:\n",
    "                        out.append((k, v)); continue\n",
    "                    if mid_letter_idx is None:\n",
    "                        if idx == 0:\n",
    "                            out.append((k, f\"<mid>{v}</mid>\"))\n",
    "                            parent_state = 'mid_letter'; mid_letter_idx = 1\n",
    "                        else:\n",
    "                            out.append((k, v))\n",
    "                    else:\n",
    "                        if idx == mid_letter_idx:\n",
    "                            out.append((k, f\"<mid>{v}</mid>\")); mid_letter_idx += 1\n",
    "                        else:\n",
    "                            out.append((k, v))\n",
    "                    continue\n",
    "            out.append((k, v))\n",
    "        return out\n",
    "\n",
    "    def add_big_to_sec4_heading_ps(self, linear_pairs):\n",
    "        out = []\n",
    "        inside_sec4 = False\n",
    "        for k, v in linear_pairs:\n",
    "            if k is None and isinstance(v, str):\n",
    "                if v.startswith(\"<SEC4 \"):\n",
    "                    inside_sec4 = True\n",
    "                    out.append((k, v)); continue\n",
    "                if v.strip() == \"</SEC4>\":\n",
    "                    inside_sec4 = False\n",
    "                    out.append((k, v)); continue\n",
    "            if inside_sec4 and k == \"p\":\n",
    "                txt = self._text_from_outer_html(v)\n",
    "                if txt in self.SEC4_HEADINGS:\n",
    "                    out.append((k, f\"<big>{v}</big>\")); continue\n",
    "            out.append((k, v))\n",
    "        return out\n",
    "\n",
    "    def _find_section_bounds(self, linear_pairs, sec_n: int):\n",
    "        open_i = close_i = None\n",
    "        open_tag = f\"<SEC{sec_n} \"\n",
    "        close_tag = f\"</SEC{sec_n}>\"\n",
    "        for i,(k,v) in enumerate(linear_pairs):\n",
    "            if k is None and isinstance(v,str) and v.startswith(open_tag):\n",
    "                open_i = i; break\n",
    "        if open_i is None:\n",
    "            return (None, None)\n",
    "        for j in range(open_i+1, len(linear_pairs)):\n",
    "            k,v = linear_pairs[j]\n",
    "            if k is None and isinstance(v,str) and v.strip() == close_tag:\n",
    "                close_i = j; break\n",
    "        return (open_i, close_i)\n",
    "\n",
    "    def nest_sec3_hierarchy(self, linear_pairs):\n",
    "        out = []\n",
    "        inside_sec3 = False\n",
    "        cur_big = None\n",
    "        cur_mid = None\n",
    "        cur_small = None\n",
    "        def close_small():\n",
    "            nonlocal cur_small\n",
    "            if cur_small is not None:\n",
    "                out.append((None, f\"</SEC3-{cur_mid}.{cur_small}>\"))\n",
    "                cur_small = None\n",
    "        def close_mid():\n",
    "            nonlocal cur_mid, cur_small\n",
    "            if cur_mid is not None:\n",
    "                close_small()\n",
    "                out.append((None, f\"</SEC3-{cur_mid}>\"))\n",
    "                cur_mid = None\n",
    "        def close_big():\n",
    "            nonlocal cur_big, cur_mid, cur_small\n",
    "            if cur_big is not None:\n",
    "                close_mid()\n",
    "                out.append((None, f\"</SEC3-{cur_big}>\"))\n",
    "                cur_big = None\n",
    "        def open_big(code_str: str):\n",
    "            nonlocal cur_big\n",
    "            close_big()\n",
    "            cur_big = code_str\n",
    "            out.append((None, f\"<SEC3-{cur_big}>\"))\n",
    "        def open_mid(code_str: str):\n",
    "            nonlocal cur_mid\n",
    "            close_mid()\n",
    "            cur_mid = code_str\n",
    "            out.append((None, f\"<SEC3-{cur_mid}>\"))\n",
    "        def open_small(letter: str):\n",
    "            nonlocal cur_small\n",
    "            close_small()\n",
    "            cur_small = letter\n",
    "            out.append((None, f\"<SEC3-{cur_mid}.{cur_small}>\"))\n",
    "        i = 0\n",
    "        while i < len(linear_pairs):\n",
    "            k, v = linear_pairs[i]\n",
    "            if k is None and isinstance(v, str):\n",
    "                if v.startswith(\"<SEC3 \"):\n",
    "                    inside_sec3 = True\n",
    "                    out.append((k, v)); i += 1; continue\n",
    "                if v.strip() == \"</SEC3>\":\n",
    "                    close_big()\n",
    "                    inside_sec3 = False\n",
    "                    out.append((k, v)); i += 1; continue\n",
    "            if not inside_sec3:\n",
    "                out.append((k, v)); i += 1; continue\n",
    "            if k == \"p\":\n",
    "                s_lower = v.lower()\n",
    "                is_big   = \"<big\"   in s_lower\n",
    "                is_mid   = \"<mid\"   in s_lower\n",
    "                is_small = \"<small\" in s_lower\n",
    "                if is_big or is_mid or is_small:\n",
    "                    txt = self._text_from_outer_html(v)\n",
    "                    if is_big:\n",
    "                        m = self.RE_MAJOR.match(txt)\n",
    "                        if m:\n",
    "                            major = m.group(\"maj\")\n",
    "                            open_big(major)\n",
    "                            out.append((k, v)); i += 1; continue\n",
    "                        out.append((k, v)); i += 1; continue\n",
    "                    if is_mid:\n",
    "                        m_num = self.RE_MID_NUM.match(txt)\n",
    "                        if m_num:\n",
    "                            code = m_num.group(\"maj\") + \".\" + m_num.group(\"sub\")\n",
    "                            open_mid(code)\n",
    "                            out.append((k, v)); i += 1; continue\n",
    "                        m_let = self.RE_LETTER.match(txt)\n",
    "                        if m_let and cur_big is not None:\n",
    "                            enum = m_let.group(\"enum\")[0]\n",
    "                            open_mid(f\"{cur_big}.{enum}\")\n",
    "                            out.append((k, v)); i += 1; continue\n",
    "                        out.append((k, v)); i += 1; continue\n",
    "                    if is_small:\n",
    "                        m = self.RE_LETTER.match(txt)\n",
    "                        if m and cur_mid is not None:\n",
    "                            enum = m.group(\"enum\")[0]\n",
    "                            open_small(enum)\n",
    "                            out.append((k, v)); i += 1; continue\n",
    "                        out.append((k, v)); i += 1; continue\n",
    "                out.append((k, v)); i += 1; continue\n",
    "            out.append((k, v)); i += 1; continue\n",
    "        if inside_sec3:\n",
    "            close_big()\n",
    "        return out\n",
    "\n",
    "    def wrap_section_by_big_linear(self, linear_pairs, sec_n: int):\n",
    "        open_i, close_i = self._find_section_bounds(linear_pairs, sec_n)\n",
    "        if open_i is None or close_i is None:\n",
    "            return linear_pairs\n",
    "        before = linear_pairs[:open_i+1]\n",
    "        inside = linear_pairs[open_i+1:close_i]\n",
    "        after  = linear_pairs[close_i:]\n",
    "        already_wrapped = any(\n",
    "            (k is None and isinstance(v,str) and v.lower().startswith(f\"<sec{sec_n}-\"))\n",
    "            for k,v in inside\n",
    "        )\n",
    "        if already_wrapped:\n",
    "            return linear_pairs\n",
    "        big_pos = []\n",
    "        for idx,(k,v) in enumerate(inside):\n",
    "            if isinstance(v,str) and v.lstrip().lower().startswith(\"<big>\"):\n",
    "                big_pos.append(idx)\n",
    "        if not big_pos:\n",
    "            return linear_pairs\n",
    "        out_inside = []\n",
    "        for b_i, start in enumerate(big_pos, start=1):\n",
    "            end = big_pos[b_i] if b_i < len(big_pos) else len(inside)\n",
    "            block = inside[start:end]\n",
    "            if not block: \n",
    "                continue\n",
    "            out_inside.append((None, f\"<SEC{sec_n}-{b_i}>\"))\n",
    "            out_inside.extend(block)\n",
    "            out_inside.append((None, f\"</SEC{sec_n}-{b_i}>\"))\n",
    "        return before + out_inside + after\n",
    "\n",
    "    # === 숫자 정규화 ===\n",
    "    def has_class_table(self, tag: Tag) -> bool:\n",
    "        classes = [(c or \"\").lower() for c in (tag.get(\"class\") or [])]\n",
    "        return any(c == \"table\" or \"table\" in c for c in classes)\n",
    "\n",
    "    def get_cell_text(self, cell: Tag) -> str:\n",
    "        txt = cell.get_text(separator=\"\", strip=True)\n",
    "        txt = txt.replace(\"\\xa0\", \" \")\n",
    "        txt = re.sub(r\"[ \\t]+\", \" \", txt)\n",
    "        return txt.strip()\n",
    "\n",
    "    def set_cell_text(self, cell: Tag, text: str):\n",
    "        cell.clear()\n",
    "        cell.append(text)\n",
    "\n",
    "    def normalize_table_numbers(self, soup: BeautifulSoup):\n",
    "        for tbl in soup.find_all(\"table\"):\n",
    "            if not self.has_class_table(tbl):\n",
    "                continue\n",
    "            rows = tbl.find_all(\"tr\", recursive=True)\n",
    "            for r_idx, tr in enumerate(rows):\n",
    "                cells = tr.find_all([\"th\",\"td\"], recursive=False)\n",
    "                for c_idx, td in enumerate(cells):\n",
    "                    if r_idx == 0 or c_idx == 0 or td.name.lower() == \"th\":\n",
    "                        continue\n",
    "                    s = self.get_cell_text(td)\n",
    "                    if not s:\n",
    "                        continue\n",
    "                    if self.RE_JUST_DASH.match(s):\n",
    "                        self.set_cell_text(td, \"NA\")\n",
    "                        continue\n",
    "                    m = self.RE_PARENS_NUM.match(s)\n",
    "                    if m:\n",
    "                        num = self.RE_COMMAS_IN_NUMBER.sub(\"\", m.group(1))\n",
    "                        self.set_cell_text(td, f\"-{num}\")\n",
    "                        continue\n",
    "                    if self.RE_NUM_WITH_COMMAS.match(s):\n",
    "                        self.set_cell_text(td, self.RE_COMMAS_IN_NUMBER.sub(\"\", s))\n",
    "                        continue\n",
    "\n",
    "    def is_inside_class_table_table(self, node: Tag) -> bool:\n",
    "        t = node.find_parent(\"table\")\n",
    "        return bool(t and self.has_class_table(t))\n",
    "\n",
    "    def normalize_text_nodes_commas(self, soup: BeautifulSoup):\n",
    "        for tag in soup.find_all(True):\n",
    "            if tag.name in self.SKIP_CONTAINERS:\n",
    "                continue\n",
    "            for child in list(tag.children):\n",
    "                if isinstance(child, NavigableString):\n",
    "                    if self.is_inside_class_table_table(tag):\n",
    "                        continue\n",
    "                    text = str(child)\n",
    "                    new_text = self.RE_COMMAS_IN_NUMBER.sub(\"\", text)\n",
    "                    if new_text != text:\n",
    "                        child.replace_with(new_text)\n",
    "\n",
    "    def write_linear_to_html(self, linear_pairs, out_path: Path):\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\n",
    "                '<!doctype html><meta charset=\"utf-8\"><title>Preprocessed</title>'\n",
    "                '<style>body{font-family:system-ui,-apple-system,Segoe UI,Roboto,Apple SD Gothic Neo,Malgun Gothic,sans-serif;line-height:1.5}'\n",
    "                'p{margin:0 0 .7em} table{margin:1em 0;border-collapse:collapse}'\n",
    "                'td,th{border:1px solid #ddd;padding:.4em}</style><body>\\n'\n",
    "            )\n",
    "            for k, v in linear_pairs:\n",
    "                f.write(v + \"\\n\")\n",
    "            f.write(\"</body>\")\n",
    "        return out_path\n",
    "\n",
    "    def process_file(self, in_path: Path, out_dir: Path = None) -> Path:\n",
    "        html_bytes = in_path.read_bytes()\n",
    "        soup = BeautifulSoup(html_bytes, \"lxml\")\n",
    "        for t in soup.find_all(self.SKIP_TAGS):\n",
    "            t.decompose()\n",
    "        self.normalize_table_numbers(soup)\n",
    "        self.normalize_text_nodes_commas(soup)\n",
    "        linear = self.flatten_to_linear(soup)\n",
    "        linear = self.trim_head_linear(linear)\n",
    "        linear = self.cut_tail_linear(linear, stop_phrase=\"외부감사 실시내용\")\n",
    "        linear = self.wrap_sections_linear(linear)\n",
    "        linear = self.split_sec1_subsections_and_drop_tables(linear)\n",
    "        linear = self.drop_first_n_tables_in_sec2(linear, n=5)\n",
    "        linear = self.drop_all_tables_in_sec4(linear)\n",
    "        linear = self.wrap_sec_titles(linear)\n",
    "        linear = self.add_big_to_sec1_subheading_ps(linear)\n",
    "        linear = self.tag_sec3_headings(linear)\n",
    "        linear = self.add_big_to_sec4_heading_ps(linear)\n",
    "        linear = self.wrap_section_by_big_linear(linear, sec_n=1)\n",
    "        linear = self.wrap_section_by_big_linear(linear, sec_n=4)\n",
    "        linear = self.nest_sec3_hierarchy(linear)\n",
    "        out_dir = out_dir or (in_path.parent / \"preprocessed\")\n",
    "        out_dir.mkdir(exist_ok=True)\n",
    "        out_path = out_dir / f\"{in_path.stem}_final.html\"\n",
    "        self.write_linear_to_html(linear, out_path)\n",
    "        return out_path\n",
    "\n",
    "    def process_directory(self, dir_path: Path, pattern=\"*.htm\"):\n",
    "        files = sorted(list(dir_path.glob(pattern)))\n",
    "        if not files:\n",
    "            print(\"No matching files found in\", dir_path)\n",
    "            return\n",
    "        out_dir = dir_path / \"preprocessed\"\n",
    "        out_dir.mkdir(exist_ok=True)\n",
    "        print(f\"Found {len(files)} files. Writing to:\", out_dir)\n",
    "        for fp in files:\n",
    "            try:\n",
    "                out_fp = self.process_file(fp, out_dir)\n",
    "                print(\"✓ Saved:\", out_fp)\n",
    "            except Exception as e:\n",
    "                print(\"✗ Failed:\", fp, \"->\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a111171e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc4a27a",
   "metadata": {},
   "source": [
    "# HtmlAuditToJson 클래스 상세 설명\n",
    "\n",
    "`HtmlAuditToJson` 클래스는 전처리된 감사보고서 HTML 파일(`*_final.html`)을 받아, 섹션/경로 정보를 보존하면서 문단, 헤딩, 표 단위로 JSON 배열(`list[dict]`)로 변환하는 기능을 제공합니다. 주요 목적은 HTML 구조를 분석하여 각 청크(문단/표/헤딩)를 메타데이터와 함께 구조화된 JSON으로 추출하는 것입니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 주요 기능 및 메서드\n",
    "\n",
    "### 1. 클래스 초기화 및 설정\n",
    "\n",
    "- **company_default**: 기본 회사명(예: \"삼성전자주식회사\")\n",
    "- **version**: 변환 버전 정보(예: \"preproc_1.0.0\")\n",
    "- **SECTION_NAMES**: 섹션 번호별 한글 섹션명 매핑\n",
    "\n",
    "### 2. 텍스트 및 표 유틸리티\n",
    "\n",
    "- **_norm_text(s)**: 텍스트 내 불필요한 공백 및 특수문자(`\\xa0`)를 정규화\n",
    "- **_cell_text(td)**: 테이블 셀의 텍스트를 정규화하여 추출\n",
    "- **_serialize_table(tbl)**: 테이블을 문자열로 직렬화(행별로 `|`로 구분), 행/열 개수 shape 정보도 함께 반환\n",
    "\n",
    "### 3. 섹션 및 제목 추출\n",
    "\n",
    "- **_nearest_section_context(node)**: HTML 노드 기준으로 가장 가까운 SEC* 섹션 컨테이너의 번호, 이름, 경로를 추출\n",
    "- **_find_prev_heading_title(node)**: 현재 노드 기준 바로 위쪽 형제에서 제목 후보(`<SEC-title>`, `<big>`, `<mid>`, `<small>` 등)를 탐색하여 반환\n",
    "- **_classify_type(tag)**: 태그가 표(`table`), 헤딩(`big`/`mid`/`small`), 일반 문단(`p`)인지 판별\n",
    "\n",
    "### 4. 파일명 기반 정보 추론\n",
    "\n",
    "- **_infer_company_from_filename(path)**: 파일명에서 회사명 추론(기본값 반환)\n",
    "- **_infer_year_from_filename(path)**: 파일명에서 연도(YYYY) 추출\n",
    "- **_infer_title_for_table(tbl)**: 표 바로 위의 헤딩 또는 첫 행의 셀 내용을 기반으로 표 제목 추론\n",
    "\n",
    "### 5. 핵심 변환 메서드\n",
    "\n",
    "#### parse_file(html_path: Path) → list[dict]\n",
    "\n",
    "- 입력: 전처리된 HTML 파일 경로\n",
    "- 처리:\n",
    "    - BeautifulSoup으로 HTML 파싱\n",
    "    - `<p>`, `<table>` 태그를 순회하며 각 청크를 추출\n",
    "    - 각 청크에 대해 섹션 정보, 제목, 타입, 표 직렬화, 인덱스, 메타데이터(회사, 연도, 파일명 등)를 dict로 생성\n",
    "    - 모든 청크를 리스트로 반환\n",
    "- 반환: JSON 직렬화 가능한 dict 리스트\n",
    "\n",
    "####\n",
    " to_json_file(html_path: Path, out_json_path: Path, indent: int = 2) → Path# HtmlAuditToJson 클래스 상세 설명\n",
    "\n",
    "`HtmlAuditToJson` 클래스는 전처리된 감사보고서 HTML 파일(`*_final.html`)을 받아, 섹션/경로 정보를 보존하면서 문단, 헤딩, 표 단위로 JSON 배열(`list[dict]`)로 변환하는 기능을 제공합니다. 주요 목적은 HTML 구조를 분석하여 각 청크(문단/표/헤딩)를 메타데이터와 함께 구조화된 JSON으로 추출하는 것입니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eaaed86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import annotations\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "from pathlib import Path\n",
    "import json, re\n",
    "\n",
    "class HtmlAuditToJson:\n",
    "    \"\"\"\n",
    "    전처리된 감사보고서 *_final.html 한 파일을 받아\n",
    "    - 섹션/경로(sec_path)를 보존하고\n",
    "    - 문단/헤딩/표 단위로\n",
    "    JSON 배열(list[dict])을 만들어 반환하거나 파일로 저장하는 변환기.\n",
    "    \"\"\"\n",
    "\n",
    "    RE_YEAR = re.compile(r\"(\\d{4})\")\n",
    "    RE_SEC_TAG = re.compile(r\"^sec(\\d+)(?:[-\\.].+)?$\", re.I)  # sec1, sec3-1, sec3-1.a\n",
    "    RE_HAS_TABLE_WORD = re.compile(r\"(표|재무상태표|현금흐름표|손익계산서|포괄손익계산서)\", re.I)\n",
    "\n",
    "    SECTION_NAMES = {1: \"감사보고서\", 2: \"재무제표\", 3: \"주석\", 4: \"감사의견\"}\n",
    "\n",
    "    def __init__(self, company_default: str = \"삼성전자주식회사\", version: str = \"preproc_1.0.0\"):\n",
    "        self.company_default = company_default\n",
    "        self.version = version\n",
    "\n",
    "    # ---------- 유틸 ----------\n",
    "\n",
    "    @staticmethod\n",
    "    def _norm_text(s: str | None) -> str:\n",
    "        if not s: return \"\"\n",
    "        s = s.replace(\"\\xa0\", \" \")\n",
    "        s = re.sub(r\"[ \\t]+\", \" \", s)\n",
    "        return s.strip()\n",
    "\n",
    "    def _cell_text(self, td: Tag) -> str:\n",
    "        return self._norm_text(td.get_text(separator=\" \", strip=True))\n",
    "\n",
    "    def _serialize_table(self, tbl: Tag) -> tuple[str, dict]:\n",
    "        rows = []\n",
    "        trs = tbl.find_all(\"tr\", recursive=True)\n",
    "        max_cols = 0\n",
    "        for tr in trs:\n",
    "            cells = tr.find_all([\"th\", \"td\"], recursive=False)\n",
    "            row = [self._cell_text(td) for td in cells]\n",
    "            max_cols = max(max_cols, len(row))\n",
    "            rows.append(\" | \".join(row))\n",
    "        txt = \"\\n\".join(rows)\n",
    "        shape = {\"rows\": len(rows), \"cols\": max_cols}\n",
    "        return txt, shape\n",
    "\n",
    "    def _nearest_section_context(self, node: Tag) -> tuple[int | None, str | None, str | None]:\n",
    "        \"\"\"\n",
    "        가장 가까운 SEC* 컨테이너 경로/섹션 번호/섹션 이름 찾기\n",
    "        \"\"\"\n",
    "        sec_path = None\n",
    "        sec_num = None\n",
    "        sec_name = None\n",
    "        for anc in [node] + list(node.parents):\n",
    "            if not isinstance(anc, Tag): continue\n",
    "            nm = (anc.name or \"\").lower()\n",
    "            if nm == \"sec-title\":\n",
    "                continue\n",
    "            m = self.RE_SEC_TAG.match(nm)\n",
    "            if m:\n",
    "                sec_path = anc.name.upper()\n",
    "                sec_num = int(m.group(1))\n",
    "                if nm == f\"sec{sec_num}\" and anc.has_attr(\"name\"):\n",
    "                    sec_name = anc.get(\"name\")\n",
    "        if sec_num is not None and sec_name is None:\n",
    "            for anc in list(node.parents):\n",
    "                if not isinstance(anc, Tag): continue\n",
    "                if (anc.name or \"\").lower() == f\"sec{sec_num}\" and anc.has_attr(\"name\"):\n",
    "                    sec_name = anc.get(\"name\")\n",
    "                    break\n",
    "        return sec_num, sec_name, sec_path\n",
    "\n",
    "    def _find_prev_heading_title(self, node: Tag) -> str | None:\n",
    "        \"\"\"\n",
    "        위쪽 형제 중 <SEC-title> 또는 <big>/<mid>/<small> 안의 <p> 를 제목으로 사용\n",
    "        \"\"\"\n",
    "        for sib in node.previous_siblings:\n",
    "            if not isinstance(sib, Tag): continue\n",
    "            n = (sib.name or \"\").lower()\n",
    "            if n == \"sec-title\":\n",
    "                t = self._norm_text(sib.get_text(\" \", strip=True))\n",
    "                return t or None\n",
    "            if n.startswith(\"sec\") and n not in (\"sec-title\",):\n",
    "                break  # 섹션 경계 만나면 중단\n",
    "            if n in (\"big\", \"mid\", \"small\"):\n",
    "                p = sib.find(\"p\")\n",
    "                if p:\n",
    "                    t = self._norm_text(p.get_text(\" \", strip=True))\n",
    "                    if t: return t\n",
    "            if n == \"p\" and sib.parent and (sib.parent.name or \"\").lower() in (\"big\", \"mid\", \"small\"):\n",
    "                t = self._norm_text(sib.get_text(\" \", strip=True))\n",
    "                if t: return t\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def _classify_type(tag: Tag) -> str:\n",
    "        if tag.name.lower() == \"table\":\n",
    "            return \"table\"\n",
    "        if tag.name.lower() == \"p\":\n",
    "            parent = tag.parent\n",
    "            if isinstance(parent, Tag) and (parent.name or \"\").lower() in (\"big\", \"mid\", \"small\"):\n",
    "                return \"heading\"\n",
    "        return \"paragraph\"\n",
    "\n",
    "    def _infer_company_from_filename(self, path: Path) -> str:\n",
    "        return self.company_default\n",
    "\n",
    "    def _infer_year_from_filename(self, path: Path) -> int | None:\n",
    "        m = self.RE_YEAR.search(path.name)\n",
    "        return int(m.group(1)) if m else None\n",
    "\n",
    "    def _infer_title_for_table(self, tbl: Tag) -> str | None:\n",
    "        t = self._find_prev_heading_title(tbl)\n",
    "        if t: return t\n",
    "        first_tr = tbl.find(\"tr\")\n",
    "        if first_tr:\n",
    "            cells = first_tr.find_all([\"th\", \"td\"], recursive=False)\n",
    "            if cells:\n",
    "                guess = self._norm_text(\" \".join(self._cell_text(c) for c in cells[:2]))\n",
    "                if self.RE_HAS_TABLE_WORD.search(guess):\n",
    "                    return guess\n",
    "        return None\n",
    "\n",
    "    # ---------- 핵심 변환 ----------\n",
    "\n",
    "    def parse_file(self, html_path: Path) -> list[dict]:\n",
    "        \"\"\"\n",
    "        입력: 전처리된 *_final.html\n",
    "        출력: 청크 dict 리스트(JSON 배열로 직렬화 가능)\n",
    "        \"\"\"\n",
    "        soup = BeautifulSoup(html_path.read_bytes(), \"lxml\")\n",
    "        body = soup.body or soup\n",
    "\n",
    "        year = self._infer_year_from_filename(html_path)\n",
    "        company = self._infer_company_from_filename(html_path)\n",
    "\n",
    "        counters: dict[str, int] = {}\n",
    "        chunks: list[dict] = []\n",
    "\n",
    "        for node in body.find_all([\"p\", \"table\"], recursive=True):\n",
    "            # 빈 <p> 스킵\n",
    "            if node.name.lower() == \"p\":\n",
    "                txt = self._norm_text(node.get_text(\" \", strip=True))\n",
    "                if not txt:\n",
    "                    continue\n",
    "            else:\n",
    "                txt = None  # 표는 직렬화 시 세팅\n",
    "\n",
    "            section_num, section_name, sec_path = self._nearest_section_context(node)\n",
    "            if section_num is None:\n",
    "                continue  # 섹션 바깥 내용은 제외\n",
    "\n",
    "            key = (sec_path or f\"SEC{section_num}\").upper()\n",
    "            chunk_index = counters.get(key, 0)\n",
    "            counters[key] = chunk_index + 1\n",
    "\n",
    "            typ = self._classify_type(node)\n",
    "\n",
    "            # 제목\n",
    "            if typ == \"heading\":\n",
    "                title = txt\n",
    "            elif typ == \"table\":\n",
    "                title = self._infer_title_for_table(node)\n",
    "            else:\n",
    "                title = self._find_prev_heading_title(node)\n",
    "\n",
    "            # 표 직렬화\n",
    "            is_table = (node.name.lower() == \"table\")\n",
    "            table_shape = None\n",
    "            if is_table:\n",
    "                txt, table_shape = self._serialize_table(node)\n",
    "\n",
    "            # ID\n",
    "            base_company = \"SAMSUNG\"\n",
    "            type_marker = \"_TABLE\" if is_table else \"\"\n",
    "            rec_id = f\"{key}_{year}_{base_company}{type_marker}_{chunk_index:04d}\"\n",
    "\n",
    "            rec = {\n",
    "                \"id\": rec_id,\n",
    "                \"text\": txt or \"\",\n",
    "                \"type\": \"table\" if is_table else (\"heading\" if typ == \"heading\" else \"paragraph\"),\n",
    "                \"section\": section_num,\n",
    "                \"section_name\": section_name or self.SECTION_NAMES.get(section_num),\n",
    "                \"sec_path\": key,\n",
    "                \"title\": title,\n",
    "                \"year\": year,\n",
    "                \"fiscal_date\": None,\n",
    "                \"auditor\": None,\n",
    "                \"company\": company,\n",
    "                \"source_file\": html_path.name,\n",
    "                \"source_path\": str(html_path),\n",
    "                \"chunk_index\": chunk_index,\n",
    "                \"char_start\": None,\n",
    "                \"char_end\": None,\n",
    "                \"lang\": \"ko\",\n",
    "                \"is_table\": is_table,\n",
    "                \"table_shape\": table_shape,\n",
    "                \"keywords\": [],\n",
    "                \"tags\": [],\n",
    "                \"version\": self.version,\n",
    "            }\n",
    "            chunks.append(rec)\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def to_json_file(self, html_path: Path, out_json_path: Path, indent: int = 2) -> Path:\n",
    "        \"\"\"\n",
    "        단일 HTML -> JSON 배열 파일로 저장\n",
    "        \"\"\"\n",
    "        chunks = self.parse_file(html_path)\n",
    "        out_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with out_json_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(chunks, f, ensure_ascii=False, indent=indent)\n",
    "        return out_json_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199ab38f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3027258",
   "metadata": {},
   "source": [
    "## 실행 파이프라인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b5d1c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 11 raw html files\n",
      "\n",
      "[1/11] Preprocess: 감사보고서_2014.htm\n",
      "  -> preprocessed: 삼성전자_감사보고서_2014_2024/preprocessed/감사보고서_2014_final.html\n",
      "  -> json saved:   삼성전자_감사보고서_2014_2024/json/감사보고서_2014_chunks.json (chunks=699)\n",
      "\n",
      "[2/11] Preprocess: 감사보고서_2015.htm\n",
      "  -> preprocessed: 삼성전자_감사보고서_2014_2024/preprocessed/감사보고서_2015_final.html\n",
      "  -> json saved:   삼성전자_감사보고서_2014_2024/json/감사보고서_2015_chunks.json (chunks=719)\n",
      "\n",
      "[3/11] Preprocess: 감사보고서_2016.htm\n",
      "  -> preprocessed: 삼성전자_감사보고서_2014_2024/preprocessed/감사보고서_2016_final.html\n",
      "  -> json saved:   삼성전자_감사보고서_2014_2024/json/감사보고서_2016_chunks.json (chunks=777)\n",
      "\n",
      "[4/11] Preprocess: 감사보고서_2017.htm\n",
      "  -> preprocessed: 삼성전자_감사보고서_2014_2024/preprocessed/감사보고서_2017_final.html\n",
      "  -> json saved:   삼성전자_감사보고서_2014_2024/json/감사보고서_2017_chunks.json (chunks=797)\n",
      "\n",
      "[5/11] Preprocess: 감사보고서_2018.htm\n",
      "  -> preprocessed: 삼성전자_감사보고서_2014_2024/preprocessed/감사보고서_2018_final.html\n",
      "  -> json saved:   삼성전자_감사보고서_2014_2024/json/감사보고서_2018_chunks.json (chunks=838)\n",
      "\n",
      "[6/11] Preprocess: 감사보고서_2019.htm\n",
      "  -> preprocessed: 삼성전자_감사보고서_2014_2024/preprocessed/감사보고서_2019_final.html\n",
      "  -> json saved:   삼성전자_감사보고서_2014_2024/json/감사보고서_2019_chunks.json (chunks=784)\n",
      "\n",
      "[7/11] Preprocess: 감사보고서_2020.htm\n",
      "  -> preprocessed: 삼성전자_감사보고서_2014_2024/preprocessed/감사보고서_2020_final.html\n",
      "  -> json saved:   삼성전자_감사보고서_2014_2024/json/감사보고서_2020_chunks.json (chunks=729)\n",
      "\n",
      "[8/11] Preprocess: 감사보고서_2021.htm\n",
      "  -> preprocessed: 삼성전자_감사보고서_2014_2024/preprocessed/감사보고서_2021_final.html\n",
      "  -> json saved:   삼성전자_감사보고서_2014_2024/json/감사보고서_2021_chunks.json (chunks=739)\n",
      "\n",
      "[9/11] Preprocess: 감사보고서_2022.htm\n",
      "  -> preprocessed: 삼성전자_감사보고서_2014_2024/preprocessed/감사보고서_2022_final.html\n",
      "  -> json saved:   삼성전자_감사보고서_2014_2024/json/감사보고서_2022_chunks.json (chunks=739)\n",
      "\n",
      "[10/11] Preprocess: 감사보고서_2023.htm\n",
      "  -> preprocessed: 삼성전자_감사보고서_2014_2024/preprocessed/감사보고서_2023_final.html\n",
      "  -> json saved:   삼성전자_감사보고서_2014_2024/json/감사보고서_2023_chunks.json (chunks=677)\n",
      "\n",
      "[11/11] Preprocess: 감사보고서_2024.htm\n",
      "  -> preprocessed: 삼성전자_감사보고서_2014_2024/preprocessed/감사보고서_2024_final.html\n",
      "  -> json saved:   삼성전자_감사보고서_2014_2024/json/감사보고서_2024_chunks.json (chunks=678)\n",
      "\n",
      "[DONE] Total files: 11, Total chunks: 8176\n",
      "[DONE] Merged JSON: 삼성전자_감사보고서_2014_2024/audit_chunks_all.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Directory pipeline:\n",
    "  HTM/HTML → IntegratedHTMLPreprocessor → *_final.html\n",
    "             → HtmlAuditToJson → *_chunks.json\n",
    "  and merge all chunks into audit_chunks_all.json (JSON array)\n",
    "\n",
    "필수: beautifulsoup4, lxml\n",
    "pip install beautifulsoup4 lxml\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import json, sys, traceback\n",
    "\n",
    "# === (1) 네가 가진 두 클래스를 import 하거나, 같은 파일에 정의했다면 이 부분 생략 ===\n",
    "# from your_module_preproc import IntegratedHTMLPreprocessor\n",
    "# from your_module_json    import HtmlAuditToJson\n",
    "\n",
    "# --- 여기서는 사용자 메시지에 있던 클래스들을 그대로 사용한다고 가정 ---\n",
    "# IntegratedHTMLPreprocessor, HtmlAuditToJson 정의가 이미 로드되어 있어야 합니다.\n",
    "\n",
    "# === 파이프라인 설정 ===\n",
    "ROOT_DIR = Path(\"./삼성전자_감사보고서_2014_2024\")\n",
    "\n",
    "# 파일 패턴\n",
    "PATTERNS = [\"*.htm\", \"*.html\"]\n",
    "\n",
    "# 출력 폴더\n",
    "PREPROC_DIR_NAME = \"preprocessed\"      # IntegratedHTMLPreprocessor가 기본으로 씀\n",
    "JSON_DIR_NAME    = \"json\"              # JSON 파일 보관\n",
    "\n",
    "# 합본 파일\n",
    "MERGED_JSON_PATH = ROOT_DIR / \"audit_chunks_all.json\"  # JSON 배열(리스트)\n",
    "\n",
    "def main():\n",
    "    if not ROOT_DIR.exists():\n",
    "        print(f\"[ERR] Root not found: {ROOT_DIR}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # 수집: *.htm / *.html\n",
    "    html_files = []\n",
    "    for pat in PATTERNS:\n",
    "        html_files.extend(sorted(ROOT_DIR.glob(pat)))\n",
    "\n",
    "    if not html_files:\n",
    "        print(f\"[ERR] No *.htm(l) files in {ROOT_DIR}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    preproc = IntegratedHTMLPreprocessor()\n",
    "    tojson  = HtmlAuditToJson(company_default=\"삼성전자주식회사\", version=\"preproc_1.0.0\")\n",
    "\n",
    "    merged_chunks = []\n",
    "    json_out_dir = ROOT_DIR / JSON_DIR_NAME\n",
    "    json_out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    print(f\"[INFO] Found {len(html_files)} raw html files\")\n",
    "    for idx, src in enumerate(html_files, 1):\n",
    "        try:\n",
    "            print(f\"\\n[{idx}/{len(html_files)}] Preprocess: {src.name}\")\n",
    "            # 1) 전처리: *_final.html 생성 (preprocessed 폴더에 저장)\n",
    "            out_final_path = preproc.process_file(src)  # returns .../preprocessed/<stem>_final.html\n",
    "            print(f\"  -> preprocessed: {out_final_path}\")\n",
    "\n",
    "            # 2) JSON 변환: *_chunks.json (json 폴더에 저장)\n",
    "            #    원본 이름을 유지해주되 suffix만 교체\n",
    "            dst_json_path = json_out_dir / (src.stem + \"_chunks.json\")\n",
    "            chunks = tojson.parse_file(out_final_path)\n",
    "            with dst_json_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(chunks, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"  -> json saved:   {dst_json_path} (chunks={len(chunks)})\")\n",
    "\n",
    "            # 3) 합본 배열에 추가\n",
    "            merged_chunks.extend(chunks)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[FAIL] {src} -> {e}\", file=sys.stderr)\n",
    "            traceback.print_exc()\n",
    "\n",
    "    # 4) 전체 합본 저장 (JSON 배열)\n",
    "    with MERGED_JSON_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(merged_chunks, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"\\n[DONE] Total files: {len(html_files)}, Total chunks: {len(merged_chunks)}\")\n",
    "    print(f\"[DONE] Merged JSON: {MERGED_JSON_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
